{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.5.1\n",
      "  Downloading torch-2.5.1-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from torch==2.5.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from torch==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from torch==2.5.1) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from torch==2.5.1) (2024.6.1)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Downloading torch-2.5.1-cp310-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1\n",
      "    Uninstalling torch-2.2.1:\n",
      "      Successfully uninstalled torch-2.2.1\n",
      "Successfully installed sympy-1.13.1 torch-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/layerskip-llama3.2-1B\"\n",
    "early_exit = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "prompt = \"typing import List\\ndef bucket_sort(A: List):\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", use_safetensors=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map = {\"\": device}, use_safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.__dict__['transformers_version'] = '4.45.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 20,\n",
       " 'max_new_tokens': None,\n",
       " 'min_length': 0,\n",
       " 'min_new_tokens': None,\n",
       " 'early_stopping': False,\n",
       " 'max_time': None,\n",
       " 'stop_strings': None,\n",
       " 'do_sample': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'penalty_alpha': None,\n",
       " 'dola_layers': None,\n",
       " 'use_cache': True,\n",
       " 'cache_implementation': None,\n",
       " 'cache_config': None,\n",
       " 'return_legacy_cache': None,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'min_p': None,\n",
       " 'typical_p': 1.0,\n",
       " 'epsilon_cutoff': 0.0,\n",
       " 'eta_cutoff': 0.0,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'encoder_repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'force_words_ids': None,\n",
       " 'renormalize_logits': False,\n",
       " 'constraints': None,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'forced_decoder_ids': None,\n",
       " 'sequence_bias': None,\n",
       " 'token_healing': False,\n",
       " 'guidance_scale': None,\n",
       " 'low_memory': None,\n",
       " 'watermarking_config': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_attentions': False,\n",
       " 'output_hidden_states': False,\n",
       " 'output_scores': False,\n",
       " 'output_logits': None,\n",
       " 'return_dict_in_generate': False,\n",
       " 'pad_token_id': None,\n",
       " 'bos_token_id': 128000,\n",
       " 'eos_token_id': 128001,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'decoder_start_token_id': None,\n",
       " 'is_assistant': False,\n",
       " 'num_assistant_tokens': 20,\n",
       " 'num_assistant_tokens_schedule': 'constant',\n",
       " 'assistant_confidence_threshold': 0.4,\n",
       " 'prompt_lookup_num_tokens': None,\n",
       " 'max_matching_ngram_size': None,\n",
       " 'generation_kwargs': {},\n",
       " '_from_model_config': True,\n",
       " '_commit_hash': '81deb0f88734409cca506bcefcfb5c6bd2667565',\n",
       " 'transformers_version': '4.45.0',\n",
       " '_original_object_hash': 5255542204128801279}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'max_length': 20, 'max_new_tokens': None, 'min_length': 0, 'min_new_tokens': None, 'early_stopping': False, 'max_time': None, 'stop_strings': None, 'do_sample': False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'dola_layers': None, 'use_cache': True, 'cache_implementation': None, 'cache_config': None, 'return_legacy_cache': None, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'min_p': None, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'forced_decoder_ids': None, 'sequence_bias': None, 'token_healing': False, 'guidance_scale': None, 'low_memory': None, 'watermarking_config': None, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'output_logits': None, 'return_dict_in_generate': False, 'pad_token_id': None, 'bos_token_id': 128000, 'eos_token_id': 128001, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'is_assistant': False, 'num_assistant_tokens': 20, 'num_assistant_tokens_schedule': 'constant', 'assistant_confidence_threshold': 0.4, 'prompt_lookup_num_tokens': None, 'max_matching_ngram_size': None, 'generation_kwargs': {}, '_from_model_config': True, '_commit_hash': '81deb0f88734409cca506bcefcfb5c6bd2667565', 'transformers_version': '4.46.0.dev0', '_original_object_hash': -2498095420294646743}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "deepcopy() got an unexpected keyword argument 'device_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m weights_memo \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m(w): w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()}\n\u001b[0;32m----> 2\u001b[0m assistant_model \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_memo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Clone main model with shared weights\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# assistant_model = assistant_model.to(model.device) \u001b[39;00m\n\u001b[1;32m      4\u001b[0m assistant_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m assistant_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[:early_exit] \u001b[38;5;66;03m# Apply early exit\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: deepcopy() got an unexpected keyword argument 'device_map'"
     ]
    }
   ],
   "source": [
    "weights_memo = {id(w): w for w in model.parameters()}\n",
    "assistant_model = deepcopy(model, memo=weights_memo) # Clone main model with shared weights\n",
    "# assistant_model = assistant_model.to(model.device) \n",
    "assistant_model.model.layers = assistant_model.model.layers[:early_exit] # Apply early exit\n",
    "del assistant_model.model.layers[early_exit:]\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# if model.generation_config.pad_token_id is None:\n",
    "#     model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config=model.generation_config, assistant_model=assistant_model, max_new_tokens=512)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformer_lens\n",
      "  Using cached transformer_lens-2.15.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (1.0.1)\n",
      "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
      "  Using cached beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
      "  Using cached better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (3.0.1)\n",
      "Collecting einops>=0.6.0 (from transformer_lens)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
      "  Using cached fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
      "  Downloading jaxtyping-0.2.38-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (2.2.2)\n",
      "Collecting rich>=12.6.0 (from transformer_lens)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch>=2.2 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (2.5.1)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.43 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (4.45.0)\n",
      "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer_lens)\n",
      "  Using cached transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typeguard<5.0,>=4.2 (from transformer_lens)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformer_lens) (4.12.2)\n",
      "Collecting wandb>=0.13.5 (from transformer_lens)\n",
      "  Using cached wandb-0.19.8-py3-none-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.13)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
      "  Downloading wadler_lindig-0.1.3-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.6.0->transformer_lens)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.2->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformers>=4.43->transformer_lens) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from transformers>=4.43->transformer_lens) (0.20.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (5.28.2)\n",
      "Collecting pydantic<3,>=2.6 (from wandb>=0.13.5->transformer_lens)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer_lens)\n",
      "  Using cached sentry_sdk-2.22.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
      "  Downloading setproctitle-1.3.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (75.8.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.18.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens)\n",
      "  Using cached pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/layer_skip/lib/python3.10/site-packages (from jinja2->torch>=2.2->transformer_lens) (3.0.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading transformer_lens-2.15.0-py3-none-any.whl (189 kB)\n",
      "Using cached beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "Using cached better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading jaxtyping-0.2.38-py3-none-any.whl (56 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Using cached wandb-0.19.8-py3-none-macosx_11_0_arm64.whl (19.9 MB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "Downloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n",
      "Downloading setproctitle-1.3.5-cp310-cp310-macosx_11_0_arm64.whl (11 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: transformers-stream-generator\n",
      "  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12500 sha256=4e8f8c3ec8bd5fb1ff806d7ab7a768f2c33e11b1e8cda4892d2779c84fee65f0\n",
      "  Stored in directory: /Users/RidamSrivastava1/Library/Caches/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\n",
      "Successfully built transformers-stream-generator\n",
      "Installing collected packages: better-abc, wadler-lindig, typeguard, smmap, setproctitle, sentry-sdk, pydantic-core, mdurl, fancy-einsum, einops, docker-pycreds, beartype, annotated-types, pydantic, markdown-it-py, jaxtyping, gitdb, rich, gitpython, wandb, transformers-stream-generator, transformer_lens\n",
      "Successfully installed annotated-types-0.7.0 beartype-0.14.1 better-abc-0.0.3 docker-pycreds-0.4.0 einops-0.8.1 fancy-einsum-0.0.3 gitdb-4.0.12 gitpython-3.1.44 jaxtyping-0.2.38 markdown-it-py-3.0.0 mdurl-0.1.2 pydantic-2.10.6 pydantic-core-2.27.2 rich-13.9.4 sentry-sdk-2.22.0 setproctitle-1.3.5 smmap-5.0.2 transformer_lens-2.15.0 transformers-stream-generator-0.0.5 typeguard-4.4.2 wadler-lindig-0.1.3 wandb-0.19.8\n"
     ]
    }
   ],
   "source": [
    "!pip install transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.loading_from_pretrained import get_pretrained_model_config\n",
    "from transformers import AutoConfig\n",
    "\n",
    "\n",
    "\n",
    "class HookedLayerSkip(HookedTransformer):\n",
    "    \"\"\"\n",
    "    A custom wrapper for the LayerSkip model loaded from a Hugging Face checkpoint.\n",
    "    It adds early-exit functionality via an assistant model and integrates with the\n",
    "    llm-transparency-tool by registering hooks for introspection.\n",
    "    \n",
    "    Parameters:\n",
    "      - exit_layer: the index of the layer at which to early exit.\n",
    "      - num_speculations: number of speculative generations (passed for completeness).\n",
    "      - checkpoint: the Hugging Face checkpoint name (e.g. \"facebook/layerskip-llama3.2-1B\").\n",
    "    \"\"\"\n",
    "    def __init__(self, config, exit_layer=None, num_speculations=None, checkpoint=None):\n",
    "        # Initialize the base HookedTransformer to get introspection capabilities.\n",
    "        super().__init__(config)\n",
    "        self.exit_layer = exit_layer\n",
    "        self.num_speculations = num_speculations\n",
    "        \n",
    "        # Load the model from Hugging Face.\n",
    "        # The checkpoint should be something like \"facebook/layerskip-llama3.2-1B\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint,\n",
    "            device_map=\"auto\",\n",
    "            use_safetensors=True,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        # Save the generation config for later use.\n",
    "        self.generation_config = self.model.generation_config\n",
    "        self.generation_config.__dict__['transformers_version'] = '4.45.0'\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name_or_path, **kwargs):\n",
    "        \"\"\"\n",
    "        Loads a pretrained LayerSkip model (from its Hugging Face checkpoint) and wraps it\n",
    "        for use with the llm-transparency-tool.\n",
    "        \n",
    "        Additional kwargs:\n",
    "          - exit_layer: for early exit.\n",
    "          - num_speculations: for speculative generation.\n",
    "        \"\"\"\n",
    "        # config = get_pretrained_model_config(model_name_or_path)\n",
    "        config = AutoConfig.from_pretrained(\"facebook/layerskip-llama3.2-1B\")\n",
    "        config = get_pretrained_model_config(\"meta-llama/Llama-3.2-1B\")\n",
    "        \n",
    "        \n",
    "        exit_layer = kwargs.pop(\"exit_layer\", None)\n",
    "        num_speculations = kwargs.pop(\"num_speculations\", None)\n",
    "        # Here model_name_or_path is assumed to be the Hugging Face checkpoint name.\n",
    "        return cls(config=config,exit_layer=exit_layer, num_speculations=num_speculations, checkpoint=model_name_or_path)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the model while registering hooks to capture intermediate\n",
    "        activations. If an exit_layer is specified, hooks are only attached to layers up to that layer.\n",
    "        \"\"\"\n",
    "        # Clear previous activations.\n",
    "        self.activations = {}\n",
    "\n",
    "        def save_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                self.activations[name] = output\n",
    "            return hook\n",
    "\n",
    "        # Locate the transformer submodule.\n",
    "        # For Llama-based models loaded via Hugging Face, the transformer body is typically in self.model.model.\n",
    "        if hasattr(self.model, \"model\"):\n",
    "            transformer = self.model.model\n",
    "        else:\n",
    "            transformer = self.model\n",
    "\n",
    "        layers = getattr(transformer, \"layers\", None)\n",
    "        if layers is None:\n",
    "            raise ValueError(\"Could not locate transformer layers for hooking.\")\n",
    "        \n",
    "        # Register hooks on each layer up to the early exit layer.\n",
    "        for idx, layer in enumerate(layers):\n",
    "            if self.exit_layer is not None and idx >= self.exit_layer:\n",
    "                break\n",
    "            layer.register_forward_hook(save_activation(f\"layer_{idx}\"))\n",
    "        \n",
    "        # Run the forward pass.\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "    def create_assistant_model(self):\n",
    "        \"\"\"\n",
    "        Creates an assistant model for early exit by deep copying the main model (with shared weights)\n",
    "        and truncating its transformer layers up to self.exit_layer.\n",
    "        \"\"\"\n",
    "        weights_memo = {id(w): w for w in self.parameters()}\n",
    "        assistant_model = deepcopy(self, memo=weights_memo)\n",
    "        # Then truncate the layers as before:\n",
    "        \n",
    "        if self.exit_layer is not None:\n",
    "            if hasattr(assistant_model, \"model\"):\n",
    "                assistant_model.model.layers = assistant_model.model.layers[:self.exit_layer]\n",
    "            else:\n",
    "                assistant_model.layers = assistant_model.layers[:self.exit_layer]\n",
    "        return assistant_model\n",
    "\n",
    "\n",
    "    def generate(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Overrides the generate method. If an early exit is specified, an assistant model is created\n",
    "        (with truncated layers) and passed via the assistant_model keyword argument.\n",
    "        \"\"\"\n",
    "        if self.exit_layer is not None:\n",
    "            kwargs[\"assistant_model\"] = self.create_assistant_model()\n",
    "        return self.model.generate(**kwargs)\n",
    "    \n",
    "    def run_with_cache(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Overrides the default run_with_cache behavior to use the assistant (early-exit)\n",
    "        model if an exit_layer is set.\n",
    "\n",
    "        This ensures that when you run analysis (via run() which calls run_with_cache)\n",
    "        on the model, it uses the truncated assistant model and captures activations from\n",
    "        only the layers up to self.exit_layer.\n",
    "        \"\"\"\n",
    "        if self.exit_layer is not None:\n",
    "            assistant_model = self.create_assistant_model()\n",
    "            return assistant_model.run_with_cache(*args, **kwargs)\n",
    "        else:\n",
    "            return super().run_with_cache(*args, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "lens_model = HookedLayerSkip.from_pretrained(\n",
    "            \"facebook/layerskip-llama3.2-1B\",\n",
    "            # hf_model=hf_model,\n",
    "            exit_layer=4,         # Set your desired early exit layer.\n",
    "            num_speculations=6,   # Set your desired number of speculations.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HookedTransformer.to_tokens() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlens_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: HookedTransformer.to_tokens() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "lens_model.to_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookPoint()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lens_model.run_with_cache()\n",
    "lens_model.hook_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HookedTransformerConfig:\n",
    "{'NTK_by_parts_factor': 32.0,\n",
    " 'NTK_by_parts_high_freq_factor': 4.0,\n",
    " 'NTK_by_parts_low_freq_factor': 1.0,\n",
    " 'act_fn': 'silu',\n",
    " 'attention_dir': 'causal',\n",
    " 'attn_only': False,\n",
    " 'attn_scale': np.float64(11.313708498984761),\n",
    " 'attn_scores_soft_cap': -1.0,\n",
    " 'attn_types': None,\n",
    " 'checkpoint_index': None,\n",
    " 'checkpoint_label_type': None,\n",
    " 'checkpoint_value': None,\n",
    " 'd_head': 128,\n",
    " 'd_mlp': 8192,\n",
    " 'd_model': 3072,\n",
    " 'd_vocab': 128256,\n",
    " 'd_vocab_out': 128256,\n",
    " 'decoder_start_token_id': None,\n",
    " 'default_prepend_bos': True,\n",
    " 'device': device(type='cuda'),\n",
    " 'dtype': torch.float32,\n",
    " 'eps': 1e-05,\n",
    " 'experts_per_token': None,\n",
    " 'final_rms': True,\n",
    " 'from_checkpoint': False,\n",
    " 'gated_mlp': True,\n",
    " 'init_mode': 'gpt2',\n",
    " 'init_weights': False,\n",
    " 'initializer_range': np.float64(0.014433756729740645),\n",
    " 'load_in_4bit': False,\n",
    " 'model_name': 'Llama-3.2-3B',\n",
    " 'n_ctx': 2048,\n",
    " 'n_devices': 1,\n",
    " 'n_heads': 24,\n",
    " 'n_key_value_heads': 8,\n",
    " 'n_layers': 28,\n",
    " 'n_params': 3170893824,\n",
    " 'normalization_type': 'RMS',\n",
    " 'num_experts': None,\n",
    " 'original_architecture': 'LlamaForCausalLM',\n",
    " 'output_logits_soft_cap': -1.0,\n",
    " 'parallel_attn_mlp': False,\n",
    " 'positional_embedding_type': 'rotary',\n",
    " 'post_embedding_ln': False,\n",
    " 'relative_attention_max_distance': None,\n",
    " 'relative_attention_num_buckets': None,\n",
    " 'rotary_adjacent_pairs': False,\n",
    " 'rotary_base': 500000.0,\n",
    " 'rotary_dim': 128,\n",
    " 'scale_attn_by_inverse_layer_idx': False,\n",
    " 'seed': None,\n",
    " 'tie_word_embeddings': False,\n",
    " 'tokenizer_name': 'meta-llama/Llama-3.2-3B',\n",
    " 'tokenizer_prepends_bos': None,\n",
    " 'trust_remote_code': False,\n",
    " 'ungroup_grouped_query_attention': False,\n",
    " 'use_NTK_by_parts_rope': True,\n",
    " 'use_attn_in': False,\n",
    " 'use_attn_result': False,\n",
    " 'use_attn_scale': True,\n",
    " 'use_hook_mlp_in': False,\n",
    " 'use_hook_tokens': False,\n",
    " 'use_local_attn': False,\n",
    " 'use_normalization_before_and_after': False,\n",
    " 'use_split_qkv_input': False,\n",
    " 'window_size': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"facebook/layerskip-llama3.2-1B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"facebook/layerskip-llama3.2-1B\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.configuration_llama.LlamaConfig'>\n"
     ]
    }
   ],
   "source": [
    "print(type(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"facebook/layerskip-llama3.2-1B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding_ln not found\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layer_skip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
